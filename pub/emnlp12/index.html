<!DOCTYPE html>
<html>
<head>
    <meta charset="utf8">
    <title>Word Salad: Relating Food Prices and Descriptions</title>
    <script src="mathjax/MathJax.js?config=TeX-AMS_HTML"></script>
    <script src="d3.js"></script>
    <link rel="stylesheet" href="style.css"/>
</head>
<body>
<article>
<header>
<h1>Word Salad: Relating Food Prices and Descriptions</h1>
<section id="authors">
<a href="/">Victor Chahuneau</a>
<a href="http://cs.cmu.edu/~kgimpel">Kevin Gimpel</a>
<a href="http://sulawesi.gsia.cmu.edu">Bryan R. Routledge</a>
<a href="#">Lily Scherlis</a>
<a href="http://cs.cmu.edu/~nasmith">Noah A. Smith</a>
</section>
</header>
<section id="abstract">
<p>We investigate the use of language in food writing, specifically on restaurant menus and in customer reviews. Our approach is to build predictive models of concrete external variables, such as restaurant menu <strong>prices</strong>. We make use of a dataset of menus and customer reviews for thousands of restaurants in several U.S. cities. By focusing on prediction tasks and doing our analysis at scale, our methodology allows quantitative, objective measurements of the words and phrases used to describe food in restaurants. We also explore interactions in language use between menu prices and <strong>sentiment</strong> as expressed in user reviews.</p>
</section>
<section id="introduction">
<h2>Introduction</h2>
<p>What words might a menu writer use to justify the high price of a steak? How does describing an item as <em>chargrilled</em> vs. <em>charbroiled</em> affect its price? When a customer writes an unfavorable review of a restaurant, how is her word choice affected by the restaurant’s prices? In this paper, we explore questions like these that relate restaurant menus, prices, and customer sentiment. Our goal is to understand how language is used in the food domain, and we direct our investigation using external variables such as restaurant menu <strong>prices</strong>.</p>
<p>We build on a thread of NLP research that seeks linguistic understanding by predicting real-world quantities from text data. Recent examples include prediction of stock volatility <span class="citation">(Kogan et al. 2009)</span> and movie revenues <span class="citation">(Joshi et al. 2010)</span>. There, prediction tasks were used for quantitative evaluation and objective model comparison, while analysis of learned models gave insight about the social process behind the data.</p>
<p>We echo this pattern here as we turn our attention to language use on restaurant menus and in user restaurant reviews. We use data from a large corpus of restaurant menus and reviews crawled from the web and formulate several prediction tasks. In addition to predicting menu prices, we also consider predicting <strong>sentiment</strong> along with price.</p>
<p>The relationship between language and sentiment is an active area of investigation <span class="citation">(Pang and Lee 2008)</span>. Much of this research has focused on customer-written reviews of goods and services, and perspectives have been gained on how sentiment is expressed in this type of informal text. In addition to sentiment, however, other variables are reflected in a reviewer’s choice of words, such as the price of the item under consideration. In this paper, we take a step toward joint modeling of multiple variables in review text, exploring connections between price and sentiment in restaurant reviews.</p>
<p>Hence this paper contributes an exploratory data analysis of language used to describe food (by its purveyors and by its consumers). While our primary goal is to understand the language used in our corpus, our findings bear relevance to economics and hospitality research as well. This paper is a step on the way to the eventual goal of using linguistic analysis to understand social phenomena like sales and consumption.</p>
</section>
<section id="related">
<h2>Related Work</h2>
<p>There are several areas of related work scattered throughout linguistics, NLP, hospitality research, and economics.</p>
<p><span class="citation">Freedman and Jurafsky (2011)</span> studied the use of language in food advertising, specifically the words on potato chip bags. They argued that, due to the ubiquity of food writing across cultures, ethnic groups, and social classes, studying the use of language for describing food can provide perspective on how different socioeconomic groups self-identify using language and how they are linguistically targeted. In particular, they showed that price affects how <q>authenticity</q> is realized in marketing language, a point we return to in <a href="#item-price-pred">§5</a>. This is an example of how price can affect how an underlying variable is expressed in language. Among other explorations in this paper, we consider how price interacts with expression of sentiment in user reviews of restaurants.</p>
<p>As mentioned above, our work is related to research in predicting real-world quantities using text data <span class="citation">(Koppel and Shtrimberg 2006; Ghose, Ipeirotis, and Sundararajan 2007; Lerman et al. 2008; Kogan et al. 2009; Joshi et al. 2010; Eisenstein et al. 2010; Eisenstein, Smith, and Xing 2011; Yogatama et al. 2011)</span>. Like much of this prior work, we aim to learn how language is used in a specific context while building models that achieve competitive performance on a quantitative prediction task.</p>
<p>Along these lines, there is recent interest in exploring the relationship between product sales and user-generated text, particularly online product reviews. For example, <span class="citation">Ghose and Ipeirotis (2011)</span> studied the sales impact of particular properties of review text, such as readability, the presence of spelling errors, and the balance between subjective and objective statements. <span class="citation">Archak et al. (2011)</span> had a similar goal but decomposed user reviews into parts describing particular aspects of the product being reviewed <span class="citation">(Hu and Liu 2004)</span>. Our paper differs from price modeling based on product reviews in several ways. We consider a large set of weakly-related products instead of a homogeneous selection of a few products, and the reviews in our dataset are not product-centered but rather describe the overall experience of visiting a restaurant. Consequently, menu items are not always mentioned in reviews and rarely appear with their exact names. This makes it difficult to directly use review features in a pricing model for individual menu items.</p>
<p>Menu planning and pricing has been studied for many years by the culinary and hospitality research community <span class="citation">(Kasavana and Smith 1982; Kelly, Kiefer, and Burdett 1994)</span>, often including recommendations for writing menu item descriptions <span class="citation">(Miller and Pavesic 1996; McVety, Ware, and Ware 2008)</span>. Their guidelines frequently include example menus from successful restaurants, but typically do not use large corpora of menus or automated analysis, as we do here. Other work focused more specifically on particular aspects of the language used on menus, such as the study by <span class="citation">Zwicky and Zwicky (1980)</span>, who made linguistic observations through manual analysis of a corpus of 200 menus.</p>
<p>Relatedly, Wansink et al. <span class="citation">(2001; 2005)</span> showed that the way that menu items are described affects customers’ perceptions and purchasing behavior. When menu items are described evocatively, customers choose them more often and report higher satisfaction with quality and value, as compared to when they are given the same items described with conventional names. Wansink et al. did not use a corpus, but rather conducted a small-scale experiment in a working cafeteria with customers and collected surveys to analyze consumer reaction. While our goals are related, our experimental approach is different, as we use automated analysis of thousands of restaurant menus and rely on a set of one million reviews as a surrogate for observing customer behavior.</p>
<p>Finally, the connection between products and prices is also a central issue in economics. However, the stunning heterogeneity in products makes empirical work challenging. For example, there are over 50,000 menu items in New York that include the word <em>chicken</em>. What is the price of chicken? This is an important practical and daunting matter when measuring inflation (e.g., Consumer Price Index is measured with a precisely-defined basket of goods). Price dispersion across goods and the variation of the goods is an important area of industrial organization economic theory. For example, economists are interested in models of search, add-on pricing, and obfuscation <span class="citation">(Baye, Morgan, and Scholten 2006; Ellison 2005)</span>.</p>
</section>
<section id="data">
<h2>Data</h2>
<p>We crawled <a href="http://www.allmenus.com">Allmenus.com</a> to gather menus for restaurants in seven U.S. cities: Boston, Chicago, Los Angeles, New York, Philadelphia, San Francisco, and Washington, D.C. Each menu includes a list of item names with optional text descriptions and prices. Most Allmenus restaurant pages contain a link to the corresponding page on <a href="http://www.yelp.com">Yelp</a> with metadata and user reviews for the restaurant, which we also collected.</p>
<p>The metadata consist of many fields for each restaurant, which can be divided into three categories: location (city, neighborhood, transit stop), services available (take-out, delivery, wifi, parking, etc.), and ambience (good for groups, noise level, attire, etc.). Also, the category of food and a price range (<span class="tex2jax_ignore">$ to $$$$</span>, indicating the price of a typical meal at the restaurant) are indicated. The user reviews include a star rating on a scale of 1 to 5.</p>
<p>The distribution of prices of individual menu items is highly skewed, with a mean of $9.22 but a median of $6.95. On average, a restaurant has 73 items on its menu with a median price of $8.69 and 119 Yelp reviews with a median rating of 3.55 stars. The star rating and price range distributions are shown in <a href="#global-stats">Figure 1</a>.</p>
<figure id="global-stats">
<div class="figure tex2jax_ignore">
<div id="pricerange-graph"></div>
<div id="stars-graph"></div>
<script src="global-stats.js"></script>
</div>
<figcaption>Frequency distributions of restaurant price ranges (left) and review ratings (right).</figcaption>
</figure>
<p>The set of restaurants was randomly split into three parts (80% for training, 10% for development, 10% for evaluation), independently for each city. The sizes of the splits and the full set of dataset statistics are provided in <a href="#data">Table 1</a>.</p>
<figure id="data-stats">
<table style="text-align:right">
<thead>
<tr><th style="text-align:left">City</th><th colspan="3" style="text-align:center">Restaurants</th><th colspan="3" style="text-align:center">Menu Items</th><th colspan="3" style="text-align:center">Reviews</th></tr>
<tr><th></th><th>train</th><th>dev.</th><th>test</th><th>train</th><th>dev.</th><th>test</th><th>train</th><th>dev.</th><th>test </th></tr>
</thead>
<tbody>
<tr><th style="text-align:left">Boston</th><td>930</td><td>107</td><td>113</td><td>63,422</td><td>8,426</td><td>8,409</td><td>80,309</td><td>10,976</td><td>11,511</td></tr>
<tr><th style="text-align:left">Chicago</th><td>804</td><td>98</td><td>100</td><td>51,480</td><td>6,633</td><td>6,939</td><td>73,251</td><td>9,582</td><td>10,965 </td></tr>
<tr><th style="text-align:left">Los Angeles</th><td>624</td><td>80</td><td>68</td><td>17,980</td><td>2,938</td><td>1,592</td><td>75,455</td><td>13,227</td><td>5,716</td></tr>
<tr><th style="text-align:left">New York</th><td>3,965</td><td>473</td><td>499</td><td>365,518</td><td>42,315</td><td>45,728</td><td>326,801</td><td>35,529</td><td>37,795 </td></tr>
<tr><th style="text-align:left">Philadelphia</th><td>1,015</td><td>129</td><td>117</td><td>83,818</td><td>11,777</td><td>9,295</td><td>52,275</td><td>7,347</td><td>5,790 </td></tr>
<tr><th style="text-align:left">San Francisco</th><td>1,908</td><td>255</td><td>234</td><td>103,954</td><td>12,871</td><td>12,510</td><td>499,984</td><td>59,378</td><td>67,010 </td></tr>
<tr><th style="text-align:left">Washington, D.C.</th><td>773</td><td>110</td><td>121</td><td>47,188</td><td>5,957</td><td>7,224</td><td>71,179</td><td>11,852</td><td>14,129 </td></tr>
</tbody>
<tfoot>
<tr><th style="text-align:left">Total</th><td>10,019</td><td>1,252</td><td>1,252</td><td>733,360</td><td>90,917</td><td>91,697</td><td>1,179,254</td><td>147,891</td><td>152,916 </td></tr>
</tfoot>
</table>
<figcaption>Dataset statistics.</figcaption>
</figure>
</section>
<section id="tasks">
<h2>Predictive Tasks</h2>
<p>We consider several prediction tasks using the dataset just described. These include predicting <strong>individual menu item prices</strong> (<a href="#item-price-pred">§5</a>), predicting the <strong>price range</strong> for each restaurant (<a href="#price-range-pred">§6</a>), and finally jointly predicting <strong>median price and sentiment</strong> for each restaurant (<a href="#joint-pred">§7</a>). To do this, we use two types of models: linear regression (<a href="#item-price-pred">§5</a> and <a href="#price-range-pred">§6</a>) and logistic regression (<a href="#joint-pred">§7</a>), both with \(\ell_1\) regularization when sparsity is desirable. We tune the regularization coefficient by choosing the value that minimizes development set loss (mean squared error and log loss, respectively).</p>
<p>For evaluation, we use mean absolute error (<abbr>MAE</abbr>) and mean relative error (<abbr>MRE</abbr>). Given a dataset \(\langle \boldsymbol{x}_i, y_i\rangle_{i=1}^N\) with inputs \(\boldsymbol{x}_i\) and outputs \(y_i\), and denoting predicted outputs by \(\hat{y}_i\), these are defined as follows:</p>
<p>\[\begin{aligned}
\mathrm{MAE} &amp; = \frac{1}{N} \sum_{i=1}^N |y_i - \hat{y}_i|\\
\mathrm{MRE} &amp; = \frac{1}{N} \sum_{i=1}^N \left|\frac{y_i - \hat{y}_i}{y_i}\right|\end{aligned}\]</p>
<p>In practice, since we model log-prices but evaluate on real prices, the final prediction is often a non-linear transformation of the output of the linear classifier of weight vector \(\boldsymbol{w}\), which we denote by: \(\hat{y}_i = f(\boldsymbol{w}^\top \boldsymbol{x}_i)\).</p>
<p>We also frequently report the total number of features available in the training set for each model (<abbr>nf</abbr>) as well as the number of non-zero feature weights following learning (<abbr>nnz</abbr>).</p>
</section>
<section id="item-price-pred">
<h2>Menu Item Price Prediction</h2>
<p>We first consider the problem of predicting the <strong>price</strong> of each item on a menu. In this case, every instance \(\boldsymbol{x}_i\) corresponds to a single item in the menu parametrized by the features detailed below and \(y_i\) is the item’s price. In this section, our models always use the logarithm of the price<sup><a href="#fn1" class="footnoteRef" id="fnref1">1</a></sup> as output values and therefore: \(\hat y_i = e^{\boldsymbol{w}^\top \boldsymbol{x}_i}\).</p>
<h4 id="baselines">Baselines</h4>
<p>We evaluate several baselines which make independent predictions for each distinct item name. The first two predict the mean or the median of the prices in the training set for a given item name, and use the overall price mean or median when a name is missing in the training set. The third baseline is an \(\ell_1\)-regularized linear regression model trained with a single binary feature for each item name in the training data. These baselines are shown as the first three rows in <a href="#item-price-pred-results">Table 2</a>.</p>
<p>We note that there is a wide variation of menu item names in the dataset, with more than 400,000 distinct names. Although we address this issue later by introducing local text features, we also performed simple normalization of the item names for all of the baselines described above. To do this normalization, we first compiled a stop word list based on the most frequent words in the item names.<sup><a href="#fn2" class="footnoteRef" id="fnref2">2</a></sup> We removed stop words and then ordered the words in each item name lexicographically, in order to collapse together items such as <em>coffee black</em> and <em>black coffee</em>. This normalization reduced the unique item name count by 40%, strengthening the baselines.</p>
<h3 id="features">Features</h3>
<p>We use \(\ell_1\)-regularized linear regression for feature-rich models. We now introduce several sets of features that we add to the normalized item names:<sup><a href="#fn3" class="footnoteRef" id="fnref3">3</a></sup></p>
<p><span class="textsc">Metadata</span>: Binary features for each restaurant metadata field mentioned above, excluding price range. A separate binary feature is included for each unique \(\langle\)field, value\(\rangle\) tuple.</p>
<p><span class="textsc">MenuNames</span>: \(n\)-grams in menu item names. We used binary features for unique unigrams, bigrams, and trigrams. Here, stop words were retained as they can be informative (e.g., <em>with</em> and <em>large</em> correlate with price).</p>
<p><span class="textsc">MenuDesc</span>: \(n\)-grams in menu item descriptions, as in <span class="textsc">MenuNames</span>.</p>
<h4 id="review-features">Review Features</h4>
<p>In addition to these features, we consider leveraging the large amount of text present in user reviews to improve predictions. We attempted to find mentions of menu items in the reviews and to include features extracted from the surrounding words in the model. Perfect item mentions being relatively rare, we consider inexact matches weighted by a coefficient measuring the degree of resemblance: we used the Dice similarity between the set of words in the sentence and in the item name. We then extracted \(n\)-gram features from this sentence, and tried several ways to use them for price prediction.</p>
<p>Given a review sentence, one option is to add the corresponding features to every item matching this sentence, with a value equal to the similarity coefficient. Another option is to select the best matching item and use the same real-valued features but only for this single item. Binary feature values can be used instead of the real-valued similarity coefficient. We also experimented with the use of part-of-speech tags in order to restrict our features to adjective and adverb \(n\)-grams instead of the full vocabulary. All of these attempts yielded negative or only slightly positive results, of which we include only one example in our experiments: the <span class="textsc">Mentions</span> feature set consists of \(n\)-grams for the best matching item with the Dice coefficient as the feature value.</p>
<p>We also tried to incorporate the reviews by using them in aggregate via predictions from a separate model; we found this approach to work better than the methods described above which all use features from the reviews directly in the regression model. In particular, we use the review features in a separate model that we will describe below (<a href="#price-range-pred">§6</a>) to predict the price range of each restaurant. The model uses unigrams, bigrams, and trigrams extracted from the reviews. We use the estimated price range (which we denote \(\widehat{\mathrm{PR}}\)) as a single additional real-valued feature for individual item price prediction.</p>
<h3 id="results">Results</h3>
<figure id="item-price-pred-results">
<table>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: center;"><abbr title="Mean Absolute Error">MAE</abbr></th>
<th style="text-align: center;"><abbr title="Mean Relative Error">MRE</abbr></th>
<th style="text-align: right;"><abbr title="Number of features">nf</abbr></th>
<th style="text-align: right;"><abbr title="Number of non-zero features">nnz</abbr></th>
</tr>
</thead>
<tbody>
<tr>
<th style="text-align: left;">Predict mean</th>
<td style="text-align: center;">3.70</td>
<td style="text-align: center;">43.32</td>
<td style="text-align: right;">n/a</td>
<td style="text-align: right;">n/a</td>
</tr>
<tr>
<th style="text-align: left;">Predict median</th>
<td style="text-align: center;">3.67</td>
<td style="text-align: center;">43.93</td>
<td style="text-align: right;">n/a</td>
<td style="text-align: right;">n/a</td>
</tr>
<tr>
<th style="text-align: left;">Regression</th>
<td style="text-align: center;">3.66</td>
<td style="text-align: center;">45.64</td>
<td style="text-align: right;">267,945</td>
<td style="text-align: right;">240,139</td>
</tr>
<tr>
<th style="text-align: left;"><span class="textsc">MenuNames</span</th>
<td style="text-align: center;">3.55</td>
<td style="text-align: center;">43.11</td>
<td style="text-align: right;">268,450</td>
<td style="text-align: right;">258,828</td>
</tr>
<tr>
<th style="text-align: left;">\(\widehat{\mathrm{PR}}\)</th>
<td style="text-align: center;">3.47</td>
<td style="text-align: center;">43.11</td>
<td style="text-align: right;">267,946</td>
<td style="text-align: right;">205,176</td>
</tr>
<tr>
<th style="text-align: left;"><span class="textsc">MenuNames</span></th>
<td style="text-align: center;">3.23</td>
<td style="text-align: center;">38.33</td>
<td style="text-align: right;">896,631</td>
<td style="text-align: right;">230,840</td>
</tr>
<tr>
<th style="text-align: left;">+ <span class="textsc">MenuDesc</span></th>
<td style="text-align: center;">3.19</td>
<td style="text-align: center;">36.23</td>
<td style="text-align: right;">1,981,787</td>
<td style="text-align: right;">151,785</td>
</tr>
<tr>
<th style="text-align: left;">+ \(\widehat{\mathrm{PR}}\)</th>
<td style="text-align: center;">3.08</td>
<td style="text-align: center;">34.51</td>
<td style="text-align: right;">1,981,788</td>
<td style="text-align: right;">140,954</td>
</tr>
<tr>
<th style="text-align: left;">+ <span class="textsc">Metadata</span></th>
<td style="text-align: center;">3.08</td>
<td style="text-align: center;">34.97</td>
<td style="text-align: right;">1,982,363</td>
<td style="text-align: right;">148,774</td>
</tr>
<tr>
<th style="text-align: left;">+ <span class="textsc">Mentions</span></th>
<td style="text-align: center;">3.06</td>
<td style="text-align: center;">34.37</td>
<td style="text-align: right;">4,959,488</td>
<td style="text-align: right;">458,462</td>
</tr>
</tbody>
</table>
<figcaption>Results for menu item price prediction. <abbr>MAE</abbr> = mean absolute error ($), <abbr>MRE</abbr> = mean relative error (%), <abbr>nf</abbr> = total number of features, <abbr>nnz</abbr> = number of features with non-zero weight.</figcaption>
</figure>
<p>Our results are shown in <a href="#item-price-pred-results">Table 2</a>. We achieve a final reduction of 50 cents in <abbr title="Mean Absolute Error">MAE</abbr> and nearly 10% in <abbr title="Mean Relative Error">MRE</abbr> compared with the baselines. Using menu name features (<span class="textsc">MenuNames</span>) brings the bulk of the improvement, though menu description features (<span class="textsc">MenuDesc</span>) and the remaining features also lead to small gains. Interestingly, as the <span class="textsc">MenuDesc</span> and \(\widehat{\mathrm{PR}}\) features are added to the model, the regularization favors more general features by selecting fewer and fewer non-zero weights.</p>
<p>While <span class="textsc">Metadata</span> features improve over the baselines when used alone, they do not lead to improved performance over the <span class="textsc">Menu*</span> + \(\widehat{\mathrm{PR}}\) features, suggesting that the text features may be able to substitute for the information in the metadata, at least for prediction of individual item prices.</p>
<p>The <span class="textsc">Mentions</span> features resulted in a small improvement in <abbr title="Mean Absolute Error">MAE</abbr> and <abbr title="Mean Relative Error">MRE</abbr>, but at the cost of expanding the model size significantly. A look at the learned feature weights reveals that most of the selected features seem more coincidental than generic (<em>rachel’s</em>, highly negative) when not totally unintuitive (<em>those suicide</em>, highest positive). This suggests that our method of extracting features from mentions is being hampered by noise. We suspect that these features could be more effective with a better method of linking menu items to mentions in review text.</p>
<h3 id="analysis">Analysis</h3>
<p>We also inspected the feature weights of our learned models. By comparing the weights of related features, we can see the relative differences in terms of contribution to menu item prices. <a href="#indiv-feats">Table 3</a> shows example feature weights, manually arranged into several categories (taken from the model with <span class="textsc">MenuNames</span> + <span class="textsc">MenuDesc</span> + \(\widehat{\mathrm{PR}}\) + <span class="textsc">Metadata</span>).</p>
<p><a href="#indiv-feats">Table 3</a> (a) shows selected features for the <q>ambience</q> field in the Yelp restaurant metadata and pane (b) lists some unigrams related to cooking methods. Pane (c) shows feature weights for \(n\)-grams often used to market menu items; we see larger weights for words targeting those who want to eat organically- or locally-grown food (<em>farmhouse</em>, <em>heirloom</em>, <em>wild</em>, and <em>hormone</em>), compared to those looking for comfort food (<em>old time favorite</em>, <em>traditional</em>, <em>real</em>, and <em>fashioned</em>). This is related to observations made by <span class="citation">Freedman and Jurafsky (2011)</span> that cheaper food is marketed by appealing to tradition and historicity, with more expensive food described in terms of naturalness, quality of ingredients, and the preparation process (e.g., <em>hand picked</em>, <em>wild caught</em>, etc.). Relatedly, in pane (e) we see that <em>real</em> mashed potatoes are expected to be cheaper than those described as <em>creamy</em> or <em>smooth</em>.</p>
<p>Pane (d) shows feature weights for trigrams containing units of chicken; we can see an ordering in terms of size (<em>bits</em> \(&lt;\) <em>cubes</em> \(&lt;\) <em>strips</em> \(&lt;\) <em>cuts</em>) as well as the price increase associated with the use of the word <em>morsels</em> in place of less refined units. We also see a difference between <em>pieces</em> and <em>pcs</em>, with the latter being frequently used to refer to entire cuts of chicken (e.g., wings, thighs, etc.) and the former more often used as a synonym for <em>chunks</em>.</p>
<p>Panes (f), (g), and (h) reveal price differences due to slight variations in word form. We find that, even though <em>crispy</em> has a higher weight than <em>crisp</em>, <em>crisp bacon</em> is more expensive than <em>crispy bacon</em>. We also find that food items prefixed with <em>roast</em> lead to more expensive prices than the similar <em>roasted</em>, except in the case of <em>pork</em>, though here the different forms may be evoking two different preparation styles.</p>
<p>Also of note is the slight difference between the nonstandard <em>mash potato</em> and <em>mashed potato</em>. We observed lower weights with other nonstandard spellings, notably <em>portobella</em> having lower weight than each of the more common spellings <em>portabella</em>, <em>portobello</em>, and <em>portabello</em>.</p>
<figure id="indiv-feats">
<div>
<div>
<h4>(a) <span class="textsc">Metadata</span>:<br/>ambience</h4>
<dl style="width: 10em">
<dt>dive-y   </dt><dd>-0.015</dd>
<dt>intimate </dt><dd>-0.013</dd>
<dt>trendy   </dt><dd>-0.012</dd>
<dt>casual   </dt><dd>-0.005</dd>
<dt>romantic </dt><dd>-0.004</dd>
<dt>classy   </dt><dd>-7e-6 </dd>
<dt>touristy </dt><dd> 0.058</dd>
<dt>upscale  </dt><dd> 0.099</dd>
</dl>
<h4>(b) <span class="textsc">MenuDesc</span>:<br/>cooking</h4>
<dl style="width: 10em">
<dt>panfried    </dt><dd>-0.094 </dd>
<dt>chargrilled </dt><dd>-0.029 </dd>
<dt>cooked      </dt><dd>-0.012 </dd>
<dt>boiled      </dt><dd>-0.006 </dd>
<dt>fried       </dt><dd>-0.005 </dd>
<dt>steamed     </dt><dd> 0.011 </dd>
<dt>charbroiled </dt><dd> 0.015 </dd>
<dt>grilled     </dt><dd> 0.022 </dd>
<dt>simmered    </dt><dd> 0.025 </dd>
<dt>roasted     </dt><dd> 0.034 </dd>
<dt>sauteed     </dt><dd> 0.034 </dd>
<dt>broiled     </dt><dd> 0.053 </dd>
<dt>seared      </dt><dd> 0.066 </dd>
<dt>braised     </dt><dd> 0.068 </dd>
<dt>stirfried   </dt><dd> 0.071 </dd>
<dt>flamebroiled </dt><dd> 0.106</dd>
</dl>
</div>
<div>
<h4>(c) <span class="textsc">MenuDesc</span>:<br/>descriptors</h4>
<dl style="width: 12em">
<dt>old time favorite </dt><dd>-0.112</dd>
<dt>fashioned </dt><dd>-0.034</dd>
<dt>line caught </dt><dd>-0.028</dd>
<dt>all natural </dt><dd>-0.028</dd>
<dt>traditional </dt><dd>-0.009</dd>
<dt>natural </dt><dd>3e-4</dd>
<dt>classic </dt><dd>0.002</dd>
<dt>free range </dt><dd>0.004</dd>
<dt>real </dt><dd>0.004</dd>
<dt>fresh </dt><dd>0.006</dd>
<dt>homemade </dt><dd>0.010</dd>
<dt>authentic </dt><dd>0.012</dd>
<dt>organic </dt><dd>0.020</dd>
<dt>specialty </dt><dd>0.025</dd>
<dt>special </dt><dd>0.033</dd>
<dt>locally </dt><dd>0.037</dd>
<dt>natural grass fed </dt><dd>0.038</dd>
<dt>artisanal </dt><dd>0.064</dd>
<dt>raised </dt><dd>0.066</dd>
<dt>heirloom </dt><dd>0.083</dd>
<dt>wild </dt><dd>0.084</dd>
<dt>hormone </dt><dd>0.085</dd>
<dt>farmed </dt><dd>0.099</dd>
<dt>hand picked </dt><dd>0.101</dd>
<dt>wild caught </dt><dd>0.116</dd>
<dt>farmhouse </dt><dd>0.133</dd>
</dl>
</div>
<div>
<h4>(d) <span class="textsc">MenuDesc</span>:<br/>_ = <em>of chicken</em></h4>
<dl style="width: 12em">
<dt>slices _</dt><dd>-0.102 </dd>
<dt>bits _</dt><dd>-0.032</dd>
<dt>cubes _</dt><dd>-0.030</dd>
<dt>pieces _</dt><dd>-0.024 </dd>
<dt>strips _</dt><dd>-0.001 </dd>
<dt>chunks _</dt><dd>0.015</dd>
<dt>morsels _</dt><dd>0.025 </dd>
<dt>pcs _</dt><dd>0.040</dd>
<dt>cuts _</dt><dd>0.042</dd>
</dl>
<h4>(g) <span class="textsc">MenuDesc</span>:<br/><em>crisp</em> vs. <em>crispy</em></h4>
<dl style="width: 12em">
<dt>crisp</dt><dd>-0.022</dd>
<dt>crispy</dt><dd>-0.011</dd>
<dt>crispy bacon</dt><dd>0.008</dd>
<dt>crisp bacon</dt><dd>0.033</dd>
</dl>
</div>
<div>
<h4>(e) <span class="textsc">MenuDesc</span>:<br/>_ = <em>potatoes</em></h4>
<dl style="width: 12em">
<dt>real mashed _</dt><dd>-0.028</dd>
<dt>mashed _</dt><dd>-0.005</dd>
<dt>creamy mashed _</dt><dd>-5e-9</dd>
<dt>smashed _</dt><dd>0.018</dd>
<dt>smooth mashed _</dt><dd>0.129</dd>
</dl>
<h4>(f) <span class="textsc">MenuDesc</span>:<br/>_ = <em>potato</em></h4>
<dl style="width: 12em">
<dt>mash _</dt><dd>-0.022</dd>
<dt> mashed</dt><dd>-0.019</dd>
</dl>
<h4>(f) <span class="textsc">MenuDesc</span>:<br/><em>roast</em> vs. <em>roasted</em></h4>
<dl style="width: 12em">
<dt>roasted</dt><dd> 0.034</dd>
<dt>roast</dt><dd> 0.040</dd>
<dt>roasted potatoes</dt><dd>0.026</dd>
<dt>roast potatoes</dt><dd>0.110</dd>
<dt>roasted chicken</dt><dd>-0.041</dd>
<dt>roast chicken</dt><dd>-0.012</dd>
<dt>roasted salmon</dt><dd>0.091</dd>
<dt>roast salmon</dt><dd>0.151</dd>
<dt>roast pork</dt><dd>-0.038</dd>
<dt>roasted pork</dt><dd>0.055</dd>
<dt>roasted tomato</dt><dd>0.010</dd>
<dt>roast tomato</dt><dd>0.026</dd>
</dl>
</div>
</div>
<figcaption>Selected features from model for menu item price prediction. See text for details.</figcaption>
</figure>
</section>
<section id="price-range-pred">
<h2>Restaurant Price Range Prediction</h2>
<p>In addition to predicting the prices of individual menu items, we also considered the task of predicting the <strong>price range</strong> listed for each restaurant on its Yelp page. The values for this field are integers from 1 to 4 and indicate the price of a typical meal from the restaurant.</p>
<p>For this task, we again train an \(\ell_1\)-regularized linear regression model with integral price ranges as the true output values \(y_i\). Each input \(\boldsymbol{x}_i\) corresponds to the feature vector for an entire restaurant. For evaluation, we round the predicted values to the nearest integer: \(\hat{y}_i =\) <span class="textsc">round</span>\((\boldsymbol{w}^\top \boldsymbol{x}_i)\) and report the corresponding mean absolute error and accuracy.</p>
<p>We compared this simple approach with an ordinal regression model <span class="citation">(McCullagh 1980)</span> trained with the same \(\ell_1\) regularizer and noted very little improvement (77.32% vs. 77.15% accuracy for <span class="textsc">Metadata</span>). Therefore, we only report in this section results for the linear regression model.</p>
<p>In addition to the feature sets used for individual menu item price prediction, we used features on reviews (<span class="textsc">Reviews</span>). Specifically, we used binary features for unigrams, bigrams, and trigrams in the full set of reviews for each restaurant. A stopword list was derived from the training data.<sup><a href="#fn4" class="footnoteRef" id="fnref4">4</a></sup> Bigrams and trigrams were filtered if they ended with stopwords. Additionally, features occurring fewer than three times in the training set were discarded.</p>
<h3 id="results-1">Results</h3>
<p>Our results for price range prediction are shown in <a href="#price-range-pred-results">Table 4</a>. Predicting the most frequent price range gave us an accuracy of 48.22%. Performance improvements were obtained by separately adding menu (<span class="textsc">Menu*</span>), metadata (<span class="textsc">Metadata</span>), and review features (<span class="textsc">Reviews</span>). Unlike individual item price prediction, the reviews were more helpful than the menu features for predicting overall price range. This is not surprising, since reviewers will often generally discuss price in their reviews. We combined metadata and review features to get our best accuracy, exceeding 80%.</p>
<p>We also wanted to perform an analysis of sentiment in the review text. To do this, we trained a logistic regression model predicting polarity for each review; we used the <span class="textsc">Reviews</span> feature set, but this time considering each review as a single training instance. The polarity of a review was determined by whether or not its star rating was greater than the average rating across all reviews in the dataset (3.7 stars). We achieved an accuracy of 87% on the test data. We omit full details of these models because the polarity prediction task for user reviews is well-known in the sentiment analysis community and our model is not an innovation over prior work <span class="citation">(Pang and Lee 2008)</span>. However, our purpose in training the model was to use the learned weights for understanding the text in the reviews.</p>
<figure id="price-range-pred-results">
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Features</th>
<th style="text-align: center;"><abbr title="Mean Absolute Error">MAE</abbr></th>
<th style="text-align: center;"><abbr title="Accuracy">Acc.</abbr></th>
<th style="text-align: right;"><abbr title="Number of features">nf</abbr></th>
<th style="text-align: right;"><abbr title="Number of non-zero features">nnz</abbr></th>
</tr>
</thead>
<tbody>
<tr>
<th style="text-align: left;">Predict mode</th>
<td style="text-align: center;">0.5421</td>
<td style="text-align: center;">48.22</td>
<td style="text-align: right;">n/a</td>
<td style="text-align: right;">n/a</td>
</tr>
<tr>
<th style="text-align: left;"><span class="textsc">Menu*</span></th>
<td style="text-align: center;">0.3875</td>
<td style="text-align: center;">66.29</td>
<td style="text-align: right;">1,910,622</td>
<td style="text-align: right;">995</td>
</tr>
<tr>
<th style="text-align: left;"><span class="textsc">Metadata</span></th>
<td style="text-align: center;">0.2372</td>
<td style="text-align: center;">77.15</td>
<td style="text-align: right;">591</td>
<td style="text-align: right;">219</td>
</tr>
<tr>
<th style="text-align: left;"><span class="textsc">Reviews</span></th>
<td style="text-align: center;">0.2172</td>
<td style="text-align: center;">79.76</td>
<td style="text-align: right;">3,027,470</td>
<td style="text-align: right;">1,567</td>
</tr>
<tr>
<th style="text-align: left;">+<span class="textsc">Metadata</span></th>
<td style="text-align: center;">0.2111</td>
<td style="text-align: center;">80.36</td>
<td style="text-align: right;">3,027,943</td>
<td style="text-align: right;">1,376</td>
</tr>
</tbody>
</table>
<figcaption>Results for restaurant price range prediction. <abbr>MAE</abbr> = mean absolute error, <abbr>Acc</abbr> = classification accuracy (%), <abbr>nf</abbr> = total number of features, <abbr>nnz</abbr> = number of features with non-zero weight.</figcaption>
</figure>
<h3 id="interpreting-reviews">Interpreting Reviews</h3>
<p>Given learned models for predicting a restaurant’s price range from its set of reviews as well as polarity for each review, we can turn the process around and use the feature weights to analyze the review text. Restricting our attention to reviews of 50–60 words, <a href="#reviews">Table 5</a> shows sample reviews from our test set that lead to various predictions of price range and sentiment.<sup><a href="#fn5" class="footnoteRef" id="fnref5">5</a></sup></p>
<p>This technique can also be useful when trying to determine the <q>true</q> star rating for a review (if provided star ratings are noisy), or to show the most positive and most negative reviews for a product within a particular star rating. The 5-point scale is merely a coarse approximation to the reviewer’s mental state; using fitted models can provide additional clues to decode the reviewer’s sentiment.</p>
<figure id="reviews">
<table style="font-size:90%">
<tr><th></th><th>← cheap</th><th></th><th style="text-align: right">expensive →</th></tr>
<tr><th style="text-align: center">\(\uparrow\)<br>\(\oplus\)</th>
<td>i love me a cheap vietnamese sandwich . mmm , pate . this place has the best ones i ’ve had in the city , and i conveniently live a few blocks away . the ladies behind the counter are always courteous and fast , and who can beat a $ 3 sandwich ?! crazy ass deli .</td>
<td>this place is tiny ! the pork buns are so tender and flavorful . i dream about these things . manila clams were awesome , not the biggest clam fan either , but i loved it . mmm 7 spice chips . i ca n’t wait to go back !</td>
<td>amazing service and desserts . nice wine list and urban decor . i went with a girlfriend and we split an entree , appetizer and dessert and they happily brought us separate portions which were just the right size . the bread is awesome , too . definitely a bit of a splurge , but worth it in moderation .</td></tr>
<tr><td></td><td>great place to get fast food that tastes good . paneer and chicken are both good . i would prefer to go thursday thru saturday night . thats when they have their good shift working . also it stays open late until 4 am on weekends . really enjoyable !</td>
<td>had some solid thai here for lunch last week . ordered the special of the day , a chicken curry . quick service and nice interior . only issue was , had a bit of a stomach ache afterwards ? prefer their sister restaurant , citizen thai and the monkey , in north beach .</td>
<td>weekday evening was quiet , not every table was filled . our waiter was amicable and friendly , which is always a plus . the coconut bread pudding was ok and very sweet . it ’s definitely a dessert plate that can be shared with a glass of wine .</td></tr>
<tr><th style="text-align: center">\(\ominus\)<br>\(\downarrow\)</th>
<td>for some reason my friend wanted me to go here with him . it was a decent standard greasy slice of pizza . it was n’t bad by any means , but it was nothing special at all . on the plus side , cheap and fast . so in summary : cheap , fast , greasy , average .</td>
<td>ugh ! the salt ! all 5 dishes we ordered were so unbearably salty , i ’d rather just have the msg . greasy , oily , salty - there is much better chinese food to be had in sf than here . i was very disappointed and wo n’t be back .</td>
<td>downhill alert ... had a decent lunch at dragon well this week marred by pretty spotty service . our waiter just did n’t have it together , forgetting to bring bowls for our split soup , our beverages , etc. . food was good but pretty pricey for what we got .</td></tr>
</table>
<figcaption>Reviews from the test set deemed by our model to have particular values of sentiment and price.</figcaption>
</figure>
<p>We can also do a more fine-grained analysis of review text by noting the contribution to the price range prediction of each position in the text stream. This is straightforward because our features are simply \(n\)-grams of the review text. In <a href="#sentence-bars">Figure 2</a>, we show the influence of each word in a review sentence on the predicted polarity (brown) and price range (yellow). The height of a bar at a given position is proportional to the sum of the feature weights for every unigram, bigram, and trigram containing the token at that position (there are at most 6 active \(n\)-grams at a position).</p>
<figure id="sentence-bars">
<div class="figure">
<iframe src="sentence-bars.html"></iframe>
</div>
<figcaption>Local (position-level) sentiment (brown) and price (yellow) estimates for two sentences in the test corpus.</figcaption>
</figure>
<p>The first example shows the smooth shift in expressed sentiment from the beginning of the sentence to the end. The second sentence is a difficult example for sentiment analysis, since there are several positive words and phrases early but the sentiment is chiefly expressed in the final clause. Our model noted the steady positive sentiment early in the sentence but identified the crucial negation due to strong negative weight on bigrams <em>fresh but</em>, <em>left me</em>, and <em>me yearning</em>. In both examples, the yellow bars show that price estimates are reflected mainly through isolated mentions of offerings and amenities (<em>drinks</em>, <em>atmosphere</em>, <em>security</em>, <em>good service</em>).</p>
</section>
<section id="joint-pred">
<h2>Joint Prediction of Price and Sentiment</h2>
<p>Although we observe no interesting correlation (\(r = 0.06\)) between median star rating and median item price in our dataset, this does not imply that sentiment and price are independent of each other.<sup><a href="#fn6" class="footnoteRef" id="fnref6">6</a></sup> We try to capture this interaction by modeling at the same time review polarity and item price: we consider the task of jointly predicting <strong>aggregate sentiment and price</strong> for a restaurant.</p>
<p>For every restaurant in our dataset, we compute its median item price \(\bar p\) and its median star rating \(\bar r\). The average of these two values for the entire dataset ($8.69 and 3.55 stars) split the plane \((\bar p, \bar r)\) in four sections: we assign each restaurant to one of these quadrants which we denote \(\downarrow \ominus\), \(\downarrow \oplus\), \(\uparrow \ominus\) and \(\uparrow \oplus\). This allows us to train a 4-class logistic regression model using the <span class="textsc">Reviews</span> feature set for each restaurant. We achieve an accuracy of 65% on the test data, but we are mainly interested in interpreting the estimated feature weights.</p>
<h3 id="analysis-1">Analysis</h3>
<p>To visualize the top feature weights learned by the model, we have to map the four weight vectors learned by the model back to the underlying two-dimensional sentiment/price space. Therefore, we compute the following values:</p>
<p>\[\begin{aligned}
\boldsymbol{w}_\$ = (\boldsymbol{w}_{\uparrow \oplus} + \boldsymbol{w}_{\uparrow \ominus}) - (\boldsymbol{w}_{\downarrow \oplus} + \boldsymbol{w}_{\downarrow \ominus}) \\
\boldsymbol{w}_\bigstar = (\boldsymbol{w}_{\uparrow \oplus} + \boldsymbol{w}_{\downarrow \oplus}) - (\boldsymbol{w}_{\uparrow \ominus} + \boldsymbol{w}_{\downarrow \ominus})\end{aligned}\]</p>
<p>We then select for display the features which are the furthest from the origin (\(\max w_\$^2+w_\bigstar^2\)) and represent the selected \(n\)-grams as points in the sentiment/price space to obtain <a href="#joint">Figure 3</a>.</p>
<figure id="joint">
<div class="figure">
<iframe src="joint.html"></iframe>
</div>
<figcaption>Top 250 features from joint prediction of price (x axis) and sentiment (y axis). The black circle is the origin. See text for details on how the coordinates for each feature were computed.</figcaption>
</figure>
<p>We notice that the spread of the sentiment values is larger, which suggests that reviews give stronger clues about consumer experience than about the cost of a typical meal. However, obvious price-related adjectives (<em>inexpensive</em> vs. <em>expensive</em>) appear in this limited selection, as well as certain phrases indicating both sentiment and price (<em>overpriced</em> vs. <em>very reasonable</em>). Other examples of note: <em>gem</em> is used in strongly-positive reviews of cheap restaurants; for expensive restaurants, reviewers use <em>highly recommended</em> or <em>amazing</em>. Also, phrases like <em>no flavor</em> and <em>manager</em> appear in negative reviews of more expensive restaurants, while <em>dirty</em> appears more often in negative reviews of cheaper restaurants.</p>
</section>
<section id="conclusion">
<h2>Conclusion</h2>
<p>We have explored linguistic relationships between food prices and customer sentiment through quantitative analysis of a large corpus of menus and reviews. We have also proposed visualization techniques to better understand what our models have learned and to see how they can be applied to new data. More broadly, this paper is an example of using extrinsic variables to drive model-building for linguistic data, and future work might explore richer extrinsic variables toward a goal of task-driven notions of semantics.</p>
</section>
<section id="acknowledgments">
<h2>Acknowledgments</h2>
<p>We thank Julie Baron, Ric Crabbe, David Garvett, Laura Gimpel, Chenxi Jiao, Elaine Lee, members of the ARK research group, and the anonymous reviewers for helpful comments that improved this paper. This research was supported in part by the NSF through CAREER grant IIS-1054319 and Sandia National Laboratories (fellowship to K. Gimpel).</p>
</section>
<footer>
<section id="references">
<h2>References</h2>
<p>Archak, N., A. Ghose, and P. G. Ipeirotis. 2011. “Deriving the Pricing Power of Product Features by Mining Consumer Reviews.” <em>Management Science</em> 57.</p>
<p>Baye, M. R., J. Morgan, and P. Scholten. 2006. “Economics and information systems; Handbooks in information systems.” In <em>Judgement under Uncertainty: Heuristics and Biases</em>, ed. T. Hendershott. Amsterdam: Elsevier.</p>
<p>Eisenstein, J., B. O’Connor, N. A. Smith, and E. P. Xing. 2010. “A Latent Variable Model for Geographic Lexical Variation.” In <em>Proc. of EMNLP</em>.</p>
<p>Eisenstein, J., N. A. Smith, and E. P. Xing. 2011. “Discovering Sociolinguistic Associations with Structured Sparsity.” In <em>Proc. of ACL</em>.</p>
<p>Ellison, G. 2005. “A Model of Add-On Pricing.” <em>Quarterly Journal of Economics</em> 120 (May): 585–637.</p>
<p>Freedman, J., and D. Jurafsky. 2011. “Authenticity in America: Class Distinctions in Potato Chip Advertising.” <em>Gastronomica</em> 11: 46–54.</p>
<p>Ghose, A., P. G. Ipeirotis, and A. Sundararajan. 2007. “Opinion Mining using Econometrics: A Case Study on Reputation Systems.” In <em>Proc. of ACL</em>.</p>
<p>Ghose, A., and P. G. Ipeirotis. 2011. “Estimating the Helpfulness and Economic Impact of Product Reviews: Mining Text and Reviewer Characteristics.” <em>IEEE Transactions on Knowledge and Data Engineering</em> 23.</p>
<p>Hu, M., and B. Liu. 2004. “Mining opinion features in customer reviews.” In <em>Proc. of AAAI</em>.</p>
<p>Joshi, M., D. Das, K. Gimpel, and N. A. Smith. 2010. “Movie Reviews and Revenues: An Experiment in Text Regression.” In <em>Proc. of NAACL</em>.</p>
<p>Kasavana, M. L., and D. I. Smith. 1982. <em>Menu Engineering: A Practical Guide to Menu Analysis</em>. Hospitality Publications.</p>
<p>Kelly, T. J., N. M. Kiefer, and K. Burdett. 1994. “A Demand-based Approach to Menu Pricing.” <em>Cornell Hotel and Restaurant Administrative Quarterly</em> 35.</p>
<p>Kogan, S., D. Levin, B. R. Routledge, J. Sagi, and N. A. Smith. 2009. “Predicting risk from financial reports with regression.” In <em>Proc. of NAACL</em>.</p>
<p>Koppel, M., and I. Shtrimberg. 2006. “Good News or Bad News? Let the Market Decide.” <em>Computing Attitude and Affect in Text: Theory and Applications</em>.</p>
<p>Lerman, K., A. Gilder, M. Dredze, and F. Pereira. 2008. “Reading the Markets: Forecasting Public Opinion of Political Candidates by News Analysis.” In <em>Proc. of COLING</em>.</p>
<p>McCullagh, P. 1980. “Regression models for ordinal data.” <em>Journal of the royal statistical society. Series B (Methodological)</em>: 109–142.</p>
<p>McVety, P. J., B. J. Ware, and C. L. Ware. 2008. <em><a href="http://books.google.com/books?id=geqEvkbx2LsC">Fundamentals of Menu Planning</a></em>. John Wiley &amp; Sons.</p>
<p>Miller, J. E., and D. V. Pavesic. 1996. <em>Menu: Pricing &amp; Strategy</em>. <em><a href="http://books.google.com/books?id=YRWlGAAACAAJ">Hospitality, Travel, and Tourism Series</a></em>. John Wiley &amp; Sons.</p>
<p>Pang, B., and L. Lee. 2008. “Opinion mining and sentiment analysis.” <em>Foundations and Trends in Information Retrieval</em> 2: 1–135.</p>
<p>Wansink, B., K. van Ittersum, and J. E. Painter. 2005. “How Descriptive Food Names Bias Sensory Perceptions in Restaurants.” <em>Food Quality and Preference</em> 16.</p>
<p>Wansink, B., J. E. Painter, and K. van Ittersum. 2001. “Descriptive Menu Labels’ Effect on Sales.” <em>Cornell Hotel and Restaurant Administrative Quarterly</em> 42.</p>
<p>Yogatama, D., M. Heilman, B. O’Connor, C. Dyer, B. R. Routledge, and N. A. Smith. 2011. “Predicting a Scientific Community’s Response to an Article.” In <em>Proc. of EMNLP</em>.</p>
<p>Zwicky, A. D., and A. M. Zwicky. 1980. “America’s National Dish: The Style of Restaurant Menus.” <em>American Speech</em> 55: 83–92.</p>
</section>
<section id="footnotes">
<ol>
<li id="fn1"><p>The price distribution is more symmetric in the log domain.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>This list can be found in the <a href="supplementary.pdf">supplementary material</a>.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>The normalized item names are present as binary features in all of our regression models.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>This list is included in the <a href="supplementary.pdf">supplementary material</a>.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>To choose the 9 reviews in the table, we took the reviews from our test set in the desired length range and computed predicted sentiment and price range for each; then we scaled the predicted price range so that its range matched that of predicted sentiment, and maximized various linear combinations of the two. This accounts for the four corners. The others were found by maximizing a linear combination of one (possibly negated) prediction minus the absolute value of the other.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>Price and sentiment are both endogenous outcomes reflecting the characteristics of the restaurant. E.g., <q>better</q> restaurants can charge higher prices.<a href="#fnref6">↩</a></p></li>
</ol>
</section>
</footer>
</article>
</body>
</html>
